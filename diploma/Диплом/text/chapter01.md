# Моделирование сетей в среде Mininet и анализ их производительности

## Обзор исследований в области моделирования в среде Mininet

Качество работы сетевого приложения определяется, в значительной степени, качествами линии передачи данных, сетевых устройств и работающих в них алгоритмах обработки потока данных.  Современные сети передачи данных не могут обеспечить нужный уровень качества обслуживания по ряду причин. Одна из них – слабая взаимосвязь физических сетевых элементов разных уровней. Контроль передачи данных лежит на каждом устройстве отдельно, что, в некоторых случаях, ведет к многочисленным потерям данных и задержкам. Консорциум Open Networking Foundation (ONF) [@onf] предложил решение данной проблемы: отделить уровень контроля передачи данных от сетевого устройства и перенести все эти заботы на некоторый контроллер. Управление данным контроллером ведется централизовано с помощью протокола OpenFlow [@openflow], что помогает установить нужный уровень предоставления услуг для конкретного приложения. Такое устройство сети консорциум ONF назвал Software-defined Networking (SDN) [@sdn]. Для изучения производительности сетей SDN методом моделирования организация ONF разработала эмулятор сети Mininet.

В работе [@article_ros] речь идет об имитационном моделировании сети SDN с помощью средств Mininet и измерении производительности сети. Были затронуты следующие показатели производительности: RTT [@rtt] для каждого направления связи, пропускная способность ветвей и направлений связи, величина задержки на сетевых элементах, загрузка портов OpenFlow Switch, элементы сети с наибольшей задержкой, число обслуженных и потерянных пакетов. Mininet хорошо подходит для задач имитационного моделирования и исследований общей эффективности работы сети, однако, для создания сети требуется умение писать программы и знание Mininet API.

Работа [@article_masdntp] представляет собой подробное руководство по взаимодействию с Mininet, как с помощью CLI [@cli], так и с помощью написания программ на языке программирования Python. Авторы показывают простоту создания сетевых топологий различной сложности, такие как Minimal, Single, Linear, Tree, Reversed. Также, в работе представлена программа, которая создает собственную сеть внутри Mininet и проводит простой анализ ее производительности. Созданная сеть детерминирована и состоит всего из 3 узлов, соединенных коммутатором. Анализ работы сети не имеет смысла, так как сеть создана, скорее, для демонстрации принципов работы с Mininet, а не для исследований. Из данной работы видно, что Mininet отлично подходит для исследований производительности сети и сетевых компонентов, благодаря своей простоте и настройке.

Идея автоматизации процесса создания топологий сети возникает сама собой, после написания двух-трех программ. Авторы работы [@article_iosctfim] описывают способ такой автоматизации. Способ основан на создании конфигурационного файла, в котором описаны основные правила адресации, создания сетевых устройств и соединений между ними. Чтение конфигурационного файла происходит в программе, написанной на языке программирования Python. Данный способ заметно ускоряет развертывание сети, избавляя исследователя от постоянных модификаций программного кода.

## Обзор исследований в области анализа производительности сетей и сетевых компонентов в среде Mininet

Простое моделирование сети не дает информации об ее производительности. Нужно провести качественный анализ работы сетевых компонентов, рассчитать пропускную способность соединения, количество потерянных и доставленных пакетов и т. д. Работа [@article_paoasdnum] является хорошим началом для изучения методов анализа производительности сети. Авторы исследуют задержки в сети при передаче данных и сравнивают показатели в HDN и SDN. Hardware Defined Network (HDN) --- привычные нам сети, где управление передачи данных лежит на устройствах сетевого и канального уровней. Работа показала, что SDN справляется с работой лучше и средняя задержка передачи данных в ней ниже (3.891 мс против 8.277 мс). Однако, стоит заметить, что здесь речь идет о передаче простого ICMP пакета, а не потока TCP/UDP трафика. 

Работа [@article_paoccmisdn] является более интересным примером анализа производительности сети в Mininet. В ней рассматриваются вопросы оценки производительности механизмов для эффективной работы с перегрузки в SDN. Авторы оценивают общую производительность сети, сравнивая ее с производительностью сети, которая использует Link Layer Discovery Protocol (LLDP) [@lldp]. Данный протокол канального уровня позволяет сетевому оборудованию оповещать оборудование, работающее в локальной сети, о своём существовании и передавать ему свои характеристики. Протокол отлично подходит для новой концепции построения сетей SDN, которую мы рассматривали ранее, так как позволяет SDN-контроллеру знать характеристики элементов сети и в зависимости от этого управлять потоками трафика. Оценка производительности основывается на трех пунктах: 

  - Уровень потери пакетов;
  - Уровень доставки пакетов;
  - Общая пропускная способность.

Данные оценки определяются для каждой итерации исследования. Всего таких итераций 4, и они имеют следующие сетевые топологии: 

  1. 1 SDN-контроллер, 4 коммутатора, 1 хост, 4 соединения;
  2. 1 SDN-контроллер, 15 коммутаторов, 16 хостов, 30 соединений;
  3. 1 SDN-контроллер, 40 коммутаторов,81 хост, 120 соединений;
  4. 1 SDN-контроллер, 85 коммутаторов, 256 хостов, 340 соединений.

В ходе работы были построен график, которые отображают зависимость сетевой характеристики от сетевой топологии для обычной сети и сети, которая использует LLDP. На графиках видно, что сеть второго типа показывает лучший уровень производительности для каждой сетевой характеристики передачи данных: ниже уровень потерь пакетов, выше уровень доставки пакетов, выше общая пропускная способность. 

Mininet отлично подходит для проведения исследований поведенческих особенностей сетевых компонентов. Однако, можно также провести исследование работоспособности и производительности сетевых протоколов и приложений. Авторы статьи [@article_apeotba] исследуют производительность алгоритма для эффективной работы с перегрузками BBRv2. Анализ работы алгоритма, презентацию и исходный код можно найти в [@bbrv2]. BBRv1 – нестандартный алгоритм управления перегрузками разработанный в Google, который не использует потерю пакетов как маркет для снижения скорости отправки. Алгоритм BBRv1 [@bbrv1], в сравнении с предшествующими алгоритмами, выдает большую пропускную способность для потоков данных при равных условиях. Одним из минусов алгоритма является его слабая совместимость с более старыми алгоритмами, используемыми в сети, и алгоритм BBRv2 создан как раз для исправления данной бреши.
В исследовании средствами Mininet создается сеть, имеющая 100 узлов отправителей и 100 узлов получателей. Отправители и получатели связаны между собой сетью из 3-х коммутаторов, каждый из который имеет свою задачу: 
- коммутатор 1: является точкой входа для узлов-отправителей, эмулирует потери и задержки данных с помощью средств NetEm [@tc_netem], соединен с коммутатором 2;
- коммутатор 2: эмулирует «узкое горлышко» между отправителями и получателями, ограничивая скорость передачи данных до 1 Гбит/с. Ограничивание передачи осуществляется с помощью дисциплины очередей TBF [@tc_tbf].
 - коммутатор 3: соединяет коммутатор 2 и хосты получатели. 

В ходе работы были исследованы и сравнены алгоритмы CUBIC [@rfc8312], BBRv1, BBRv2 на такие сетевые характеристики: пропускная способность, индекс справедливости [@rfc5166], сосуществование. Результаты показывают, что BBRv2 обеспечивает лучшее сосуществование с потоками, использующие алгоритм CUBIC, по сравнению со своим предшественником. Кроме того, BBRv2 способен обеспечить более справедливую долю пропускной способности по сравнению с BBRv1, когда сетевые условия, такие как пропускная способность и задержка, динамически изменяются. 

## Алгоритмы управления перегрузками в сети

Часто в теории сетей встречается термин перегрузка. Данное понятие характеризуется уменьшением пропускной способности сети вследствие возникновения заторов на интерфейсах коммутаторов. Пакеты, поступая на интерфейс, образуют очередь, если пакет не может быть передан дальше по линии передачи данных. Это происходит из-за несоответствий входной и выходной пропускной способности линии передачи данных. Данная очередь хранится в буфере коммутатора и непосредственно влияет на передачу данных. Например, если памяти много и очередь длинная, то пакет вынужден ждать значительное время, прежде чем он будет передан. И наоборот, если очередь мала, то пакет быстрее пойдет дальше по сети. Однако, в таком случае, пакеты, не помещающиеся в очередь, будут попросту отброшены и не дойдут до получателя.

Исследователи в области сетевых технологий постоянно ищут баланс между временем доставки пакета и пропускной способностью сети, в которой сосуществует много пользователей, единственным желанием которых является отправка наибольшего количества данных за наименьшее количество времени. В данный момент существует множество алгоритмов, удовлетворяющих, хотя бы отчасти, эти запросы.

Работа [@rfc2001] описывает механизмы, используемые в алгоритмах работы с перегрузками. Данные механизмы именуются как медленный старт, быстрая повторная передача, быстрое восстановление. Данные механизмы завязаны на увеличении/уменьшении окна перегрузки (некоторый набор сегментов данных tcp) в зависимости от того, был ли переданный пакет из сегмента потерян. Определяется потеря пакета как сама потеря или же как получение трех дубликатов получения пакета.

Медленный старт – фаза наращивания передачи данных, при которой окно перегрузки увеличивается экспоненциально каждый раз, когда получено подтверждение. Это происходит до тех пор, пока для какого-то сегмента не будет получено подтверждение или будет достигнуто какое-то заданное пороговое значение таймера. Как только произойдет событие потери пакета, задается пороговое значение медленного старта, равное половине окна перегрузки.

Быстрое восстановление - вариация алгоритма медленного старта, которая использует быструю повторную передачу последующей фазой предотвращения перегрузки. В алгоритме быстрого восстановления, во избежание перегрузки, когда пакет не получен, размер окна перегрузки сводится к порогу медленного старта, а не к меньшему начальному значению.

Механизмы медленного старта и быстрого восстановления, включающий в себя механизм быстрой повторной передачи, используются, например, в алгоритме TCP Reno. Помимо прочего, в TCP Reno имеется механизм аддитивного увеличение и мультипликативного уменьшения, который включается после потери пакета в фазе медленного старта.

Однако, не все алгоритмы работы с перегрузками используют как маркер перегрузки в сети потерю пакета. При взаимодействии потоков данных часто полезно использовать маркер перегрузки значение пропускной способности и RTT, не доходя до появления потерь. Данный метод подробно описан в работе [@bbrv1] и реализован в алгоритме TCP BBR и расширен в TCP BBRv2 [@bbrv2]. Данный алгоритм позволяет держать пропускную способность в оптимальном значении, постоянно исследуя изменения RTT (изменение возможно только в случае физического изменения длины передачи данных) и текущей пропускной способности. Начинается работа алгоритма с фазы медленного старта, находится оптимальное значение RTT и BW (пропускная способность) и поочередно на каждом шаге происходит проверка на улучшение сетевых показателей. Например, стало ли нам доступно большая полоса пропускания данных, можем ли мы получить меньший RTT и т. д. Данный алгоритм требует больших вычислительных ресурсов в ряду своей сложности, однако, дает приближенные к оптимальным значения характеристик BW и RTT при передаче данных. Следует так же заметить, что TCP BBR вытесняет потоки данных, использующие другие версии алгоритмов управления перегрузками, которые используют потерю пакетов как метрику перегрузки. Данная проблема была решена в алгоритме TCP BBRv2.