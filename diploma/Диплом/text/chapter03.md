# Моделирование модуля активного управления трафиком сети передачи данных

## Создание модуля для среды Mininet

В главе 2 уже был рассмотрен способ моделирования сети в среде Mininet. Данный способ включает в себя создание программы на языке программирования Python. В программе описывается топология сети, манипуляции с сетью, ее старт и завершение. Написав пару таких сетей, можно столкнуться с мыслью, что части кода идентичны друг другу, а каждое редактирование и исследование сети занимает время. Можно написать автоматизированное решение, которое требует только задание нужных сетевых элементов и их конфигураций, а программа сама считает данные, создаст сеть, построит графики сетевых характеристик и т. д. Данная идея будет описана далее в этой главе. 

Для начала требуется выбрать формат конфигурационного файла. Выбор формата конфигурационного файла производится из собственных предпочтений и удобства анализа файла. В данной работе будет использован формат toml [@toml]. Определим объекты сети, которые мы будем описывать в конфигурационном файле: хосты, коммутаторы, соединения, мониторинговые характеристики, команды qdisc для интерфейсов коммутатора. 

Имея подобный конфигурационный toml-файл, его можно прочесть с помощью средств Python, обработать и положить требуемые значения в объекты. Такой подход позволяет строить сколь угодно большие топологии без правки логики приложения. На основе данного файла можно построить программу, использующую диаграмму активностей, показанную на рис.[-@fig:30001]

![Диаграмма активностей приложения](act_dia.png){ #fig:30001 width=70% }

Программу можно разделить на такие модули как: мониторинг сети, построение графиков сетевых характеристик, модуль модели сети, который включает в себя топологию сети и два предыдущих элемента. Исходя из этого можно построить диаграмму классов приложения, показанную на рис.[-@fig:30002].

![Диаграмма классов приложения](class_dia.png){ #fig:30002 width=70% }

Главным классом, включающим в себя все остальные, является CustomModel. В методе simulation создаются объекты классов Monitor, mininet.net.Mininet, CustomTopology и NetStatsPlotter. Рассмотрим подробнее классы, создаваемые в методе simulation:

- mininet.net.Minet — предоставляемый Mininet API класс, отвечающий за создание сети с топологией, указанной в CustomTopology;
- CustomTopology — класс топологии сети;
- Monitor — класс, в котором происходят замеры сетевых характеристик исследуемой сети;
- NetStatsPlotter — класс, объект которого занимается построением графиков сетевых характеристик.

Код классов NetStatsPlotter, Monitor, CustomTopology и CustomModel находятся в [приложении A](#appendix1), [приложении B](#appendix2), [приложении C](#appendix3) и [приложении D](#appendix4) соответственно. 

Точкой входа в данную программу будет файл main.py. Содержимое данного файла представлено ниже:

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos
]
{python}
'''
#!/usr/bin/python3.8

import argparse
from model.CustomModel import CustomModel

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    h = "Файл конфигурации"
    parser.add_argument('-c', '--config', type=str, help=h)
    args = parser.parse_args()
    if args.config:
        top = CustomModel()
        top.configure_model(args.config)
        top.simulation()
    else:
        print("Введите название конфиг-файла")

\end{minted}

Это обычный скрипт на языке программирования Python, который создает модель нашей сети из конфигурационного файла, переданного параметром командной строки. Запустить данный скрипт можно с помощью команды

\begin{minted}[breaklines]{bash}
sudo ./main.py -c config/pfifo_config.toml
\end{minted}

Параметр **-c** отвечает за местоположение конфигурационного файла. Естественно, перед запуском скрипта требуется задать право на исполнение файла. Делается это с помощью команды

\begin{minted}[breaklines]{bash}
chmod +x main.py
\end{minted}

После запуска программы в каталоге приложения появится директория с именем, которое было указано в toml-файле. В ней содержатся графики изменения сетевых характеристик и сырые данные, которые были обработаны объектом класса NetStatsPlotter.

## Тестирование программного модуля

У нас имеется готовая программа в наличии и нам следует ее протестировать. Для начала требуется описать сеть, заполнить конфигурационный файл по характеристикам заданной сети, запустить программу и исследовать сетевые характеристики сети передачи данных. 

Пусть у нас имеется сеть, заданная топологией, приведенной на рис.[-@fig:30003].

![Топология исследуемой сети](example_topo.png){ #fig:30003 width=70% }

Предполагается, что в данной сети будут действовать следующие правила:

- скорость передачи данных ограничена;
- максимальная пропускная способность соединения h1-s1 равна 100 Мбит/с;
- максимальная пропускная способность соединения s2-h2 равна 50 Мбит/с;
- на коммутаторе s2 стоит дисциплина обработки очередей FIFO с максимальным количеством пакетов, равным 60;
- потери в сети составляют 0.001%;
- задержка имеет нормальное распределение с математическим ожиданием в
30 мс и с дисперсией в 7 мс.
- в сети работает алгоритм для работы с перегрузками TCP Reno [@rfc2001].

Опишем данные характеристики в конфигурационном файле.

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos
]
{kconfig}
'''
# device settings
[devices]
    [devices.h1]
        name = "h1"
        ip = "10.0.0.1"
        cmd = [
            "sysctl -w net.ipv4.tcp_congestion_control=reno"
        ]
    [devices.h2]
        name = "h2"
        ip = "10.0.0.2"
        cmd = [
            "sysctl -w net.ipv4.tcp_congestion_control=reno"
        ]

# switch settings
[switches]
    [switches.s1]
        name = "s1"
    [switches.s2]
        name = "s2"

# link settings
[links]
pairs = [
    ["h1", "s1"],
    ["s1", "s2"],
    ["s2", "h2"]
]
cmd = [
    "tc qdisc replace dev s1-eth2 root handle 10: tbf rate 100mbit burst 50000 limit 150000",
    "tc qdisc add dev s1-eth2 parent 10: handle 20: netem loss 0.001% delay 30ms 7ms distribution normal",
    "tc qdisc replace dev s1-eth1 root handle 10: tbf rate 100mbit burst 50000 limit 150000",
    "tc qdisc add dev s1-eth1 parent 10: handle 20: netem loss 0.001% delay 30ms 7ms distribution normal",
    "tc qdisc replace dev s2-eth2 root handle 10: tbf rate 50mbit burst 25000 limit 75000",
    "tc qdisc add dev s2-eth2 parent 10: handle 15: pfifo limit 30",
    "tc qdisc replace dev s2-eth1 root handle 10: tbf rate 50mbit burst 25000 limit 75000"
]

[monitoring]
monitoring_time = 60
monitoring_interval = 0.005
host_client = "h1"
host_server = "h2"
interface = "s2-eth2"
iperf_file_name = "iperf.json"
iperf_flags = ""
queue_data_file_name = "qlen.data"
plots_dir = "plots_dir_first"
\end{minted}

В представленном toml-файле раздел **devices** отвечает за настройку конечных узлов сети, раздел **switches** отвечает за настройку коммутаторов, а раздел **links** отвечает за настройку соединений узлов сети и конфигурацию интерфейсов коммутаторов.

На хостах указывается ip-адрес, имя хоста и алгоритм работы с перегрузками. На коммутаторах прописывается только имя, однако, список настроек можно расширить, изменив программную логику в классе с топологией. В разделе **links** явно указывается, какие пары сетевых устройств соединяются, и команды, которые настраивают дисциплину очередей на интерфейсах. Важно уточнить, что соединения на коммутаторе происходят последовательно, т. е. если первым идет подключение h1-s1, то интерфейсом на коммутаторе, отвечающим за данное соединение, является интерфейс s1-eth1. Если, например, подключить еще одно устройство к коммутатору, то интерфейсом коммутатора в соединении будет s1-eth2, и так далее. Данный момент следует учитывать при установке правил дисциплин очередей на интерфейсах.

Также, в toml-файле имеется блок **monitoring**, в котором заданы следующие параметры: 

- monitoring_time --- время мониторинга сети в секундах;
- monitoring_interval --- интервалы между замерами длины очереди в секундах;
- host_client --- узел, который будет отправлять данные;
- host_server --- узел, который будет принимать данные;
- interface --- интерфейс, на котором будет мониторится размер очереди;
- iperf_file_name --- имя файла с отчетом мониторинга iperf;
- iperf_flags --- iperf-флаги клиента;
- queue_data_file_name --- имя файла с отчетом мониторинга длины очереди;
- plots_dir --- директория со всеми графиками сетевых характеристик.

Все параметры являются обязательными.

Запуск программы показан рис. [-@fig:30004].

![Запуск программы](ch03_01/prog_start1.png){ #fig:30004 width=70% }

Рассмотрим получившиеся графики сетевых характеристик:

- На рис. [-@fig:30005] приведен график, показывающий динамику объема переданных
данных за время моделирования.
- На рис. [-@fig:30006] приведен график изменения значений окна перегрузки TCP Reno (CWND).
- Динамика значений RTT (измеряется в миллисекундах) демонстрируется на рис [-@fig:30007].
- График отклонений значений RTT приведен на рис. [-@fig:30008].
- Динамика изменения пропускной способности показана на рис. [-@fig:30009].
- Изменение длины очереди на интерфейсе s2-eth2 показано на рис. [-@fig:30010]
- Изменение значения количества повторно переданных пакетов показано на рис. [-@fig:30011]

![График изменения количества переданных данных с течением времени](ch03_01/bytes.png){ #fig:30005 width=70% }

![График изменения значения окна перегрузки с течением времени при использовании алгоритма TCP Reno](ch03_01/cwnd.png){ #fig:30006 width=70% }

![График изменения значения RTT с течением времени](ch03_01/rtt.png){ #fig:30007 width=70% }

![График изменения значения вариации RTT с течением времени](ch03_01/rttvar.png){ #fig:30008 width=70% }

![График изменения значения пропускной способности с течением времени](ch03_01/throughput.png){ #fig:30009 width=70% }

![График изменения длины очереди на интерфейсе s2-eth2](ch03_01/queue_len.png){ #fig:30010 width=70% }

![График изменения значения повторно переданных пакетов во времени](ch03_01/retransmits.png){ #fig:30011 width=70% }

## Пример простой сети с несколькими хостами

Сеть, в которой существуют только хост-сервер и хост-клиент встречаются довольно редко. Требуется усложнить модель сети, добавив в нее несколько хостов, которые в некоторый промежуток времени начинают передавать данные на сервер. Реализовать временные задержки довольно легко, если вспомнить про существование скриптовых файлов, которые можно запустить на хостах. Внутри данного файла будет настройка хоста, подключение к серверу и симуляция задержек во времени с помощью утилиты sleep [@sleep].

Пусть у нас имеется сеть, заданная топологией, приведенной на рис.[-@fig:30012].

![Топология исследуемой сети](ch03_02/example_topo.png){ #fig:30012 width=70% }

Предполагается, что в данной сети будут действовать следующие правила:

- скорость передачи данных ограничена;
- максимальная пропускная способность соединения s4-s1 равна 15 Мбит/с;
- максимальная пропускная способность соединения s4-s2 равна 10 Мбит/с;
- максимальная пропускная способность соединения s4-s3 равна 20 Мбит/с;
- максимальная пропускная способность соединения s4-h4 равна 20 Мбит/с;
- на коммутаторе s4 стоит дисциплина обработки очередей FIFO с максимальным количеством пакетов, равным 45;
- потери в сети составляют 0.001%;
- задержка имеет нормальное распределение с математическим ожиданием в
10 мс и с дисперсией в 3 мс.
- в сети работает алгоритм для работы с перегрузками TCP Reno.

Предполагается, что h2 начнет передавать данные на сервер через 10 секунд после старта сети и время передачи равняется 20 секундам, а h3 начнет передавать данные через 20 секунд с таким же временем передачи. В данной сети коммутаторы s1, s2, s3 существуют лишь для технической реализации сетевых задержек, потерь и ограничения скорости передачи данных.

Опишем данные характеристики в конфигурационном файле.

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos
]
{kconfig}
'''
# device settings
[devices]
    [devices.h1]
        name = "h1"
        ip = "10.0.0.1"
        cmd = [
            "sysctl -w net.ipv4.tcp_congestion_control=reno"
        ]
    [devices.h2]
        name = "h2"
        ip = "10.0.0.2"
        cmd = [
            "./config/host_configs/h2.sh"
        ]
    [devices.h3]
        name = "h3"
        ip = "10.0.0.3"
        cmd = [
            "./config/host_configs/h3.sh"
        ]
    [devices.h4]
        name = "h4"
        ip = "10.0.0.4"
        cmd = [
            "iperf3 -s -p 7778 -1",
            "iperf3 -s -p 7779 -1",
        ]


# switch settings
[switches]
    [switches.s1]
        name = "s1"
    [switches.s2]
        name = "s2"
    [switches.s3]
        name = "s3"
    [switches.s4]
        name = "s4"


# link settings
[links]
pairs = [
    ["h1", "s1"],
    ["h2", "s2"],
    ["h3", "s3"],
    ["s1", "s4"],
    ["s2", "s4"],
    ["s3", "s4"],
    ["s4", "h4"]
]
cmd = [
    "tc qdisc replace dev s4-eth4 root handle 10: tbf rate 20mbit burst 10000 limit 30000",
    "tc qdisc replace dev s4-eth1 root handle 10: tbf rate 20mbit burst 10000 limit 30000",
    "tc qdisc replace dev s4-eth2 root handle 10: tbf rate 20mbit burst 10000 limit 30000",
    "tc qdisc replace dev s4-eth3 root handle 10: tbf rate 20mbit burst 10000 limit 30000",
    "tc qdisc add dev s4-eth4 parent 10: handle 15: pfifo limit 45",

    "tc qdisc replace dev s1-eth2 root handle 10: tbf rate 15mbit burst 7500 limit 22500",
    "tc qdisc replace dev s2-eth2 root handle 10: tbf rate 10mbit burst 5000 limit 15000",
    "tc qdisc replace dev s3-eth2 root handle 10: tbf rate 20mbit burst 10000 limit 30000",
    
    "tc qdisc replace dev s1-eth1 root handle 10: tbf rate 15mbit burst 7500 limit 22500",
    "tc qdisc replace dev s2-eth1 root handle 10: tbf rate 10mbit burst 5000 limit 15000",
    "tc qdisc replace dev s3-eth1 root handle 10: tbf rate 20mbit burst 10000 limit 30000",
    

    "tc qdisc add dev s1-eth2 parent 10: handle 20: netem loss 0.001% delay 10ms 3ms distribution normal",
    "tc qdisc add dev s2-eth2 parent 10: handle 20: netem loss 0.001% delay 10ms 3ms distribution normal",
    "tc qdisc add dev s3-eth2 parent 10: handle 20: netem loss 0.001% delay 10ms 3ms distribution normal",

    "tc qdisc add dev s1-eth1 parent 10: handle 20: netem loss 0.001% delay 10ms 3ms distribution normal",
    "tc qdisc add dev s2-eth1 parent 10: handle 20: netem loss 0.001% delay 10ms 3ms distribution normal",
    "tc qdisc add dev s3-eth1 parent 10: handle 20: netem loss 0.001% delay 10ms 3ms distribution normal"
    
]

[monitoring]
monitoring_time = 60
monitoring_interval = 0.05
host_client = "h1"
host_server = "h4"
interface = "s4-eth4"
iperf_file_name = "iperf.json"
iperf_flags = "-b 15mbit"
queue_data_file_name = "qlen.data"
plots_dir = "plots_dir_second"

\end{minted}

Видно, что здесь, в разделе настроек хостов, у хоста h4 запускаются 2 iperf-сервера. К ним, через скрипт-файлы, подключатся хосты h2 и h3. 

Скрипт-файл для хоста h2:

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos
]
{bash}
#!/bin/bash
sysctl -w net.ipv4.tcp_congestion_control=reno
sleep 10
iperf3 -c 10.0.0.4 -p 7778 -t 20
\end{minted}

Скрипт-файл для хоста h3:

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos
]
{bash}
#!/bin/bash
sysctl -w net.ipv4.tcp_congestion_control=reno
sleep 20
iperf3 -c 10.0.0.4 -p 7779 -t 20
\end{minted}

Запустим данную сеть и рассмотрим наиболее интересующие нас графики сетевых характеристик.

Динамика изменения пропускной способности показана на рис. [-@fig:30015]. Видно, что наименьшее значение пропускной способности достигалось в промежутке, когда одновременно все хосты передавали данные на сервер.

![График изменения значения пропускной способности с течением времени](ch03_02/throughput.png){ #fig:30015 width=70% }

На рис. [-@fig:30013] приведен график изменения значений окна перегрузки TCP Reno (CWND). На графике видно, что наибольшее значение CWND достигается только тогда, когда 1 хост передает данные на сервер, а все остальные молчат.

![График изменения значения окна перегрузки с течением времени при использовании алгоритма TCP Reno](ch03_02/cwnd.png){ #fig:30013 width=70% }

Динамика значений RTT демонстрируется на рис [-@fig:30014]. Видно, RTT увеличивается пропорционально росту окна перегрузки.

![График изменения значения RTT с течением времени](ch03_02/rtt.png){ #fig:30014 width=70% }

